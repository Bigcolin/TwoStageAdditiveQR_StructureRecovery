module additiveQR
include("supportFuncs.ji")
include("localLinear_quantReg.ji")
include("DGP.ji")
using BasicBSpline

struct bsplElems
    pBasis::Int
    nBasis::Int
    knot::Array{Float64, 1}
    function bsplElems(nBasis::Int, pBasis::Int)
        knot = LinRange(-2, 2, pBasis + nBasis + 1)
        new(pBasis, nBasis, knot)
    end

    function bsplElems(knot::Array{Float64, 1}, pBasis::Int)
        nBasis = length(knot) - pBasis - 1
        new(pBasis, nBasis, knot)
    end
end

function assemblePx(X, p)
    n, d = size(X)
    Ak = []
    for k in 1:d
        ak = p.(X[:,k])
        ak = hcat(ak...)
        push!(Ak, ak')
    end
    A = hcat(Ak...)
    return A , Ak
end

struct model
    basisElems::bsplElems
    pfuncs
    Mx::Array{Float64, 2}
    coef::Array{Float64, 1}
    intercept::Float64
    # qbic::Float64
    trainData::data
    y_min::Float64
    y_max::Float64
    tau::Float64
    function model(splElems::bsplElems, trainData::data, tau::Float64)
        ρ(x) = rho(x, tau)
        nBasis = splElems.nBasis
        pBasis = splElems.pBasis
        knots = KnotVector(splElems.knot)
        bsp = BSplineSpace{pBasis}(knots)
        pfuncs(t) = [bsplinebasis(bsp,i,t) for i in 1:BasicBSpline.dim(bsp)]
        n, d = size(trainData.X)
        A, Ak = assemblePx(trainData.X, pfuncs)
        θ_0 = rand(d*nBasis + 1)
        # func(θ) = sum(ρ.(trainData.y - A*θ))./n
        # res = optimfunc(func, θ_0)
        res = localLinear_quantReg.qr_solver(trainData.y, A, tau, 0)
        # QBIC: determined by basis parameters
        # qbic = n*log(n*res.minimum) + 2log(n)*nBasis
        coef = res[2:end]
        intercept = res[1]
        M = zeros(n, d)
        coefm = reshape(coef, nBasis, d)
        for k in 1:d
            M[:,k] = Ak[k] * coefm[:,k]
        end
        y_min = minimum(trainData.y) 
        y_max = maximum(trainData.y)
        new(splElems, pfuncs, M, coef, intercept, trainData, y_min, y_max, tau)
    end

end

function predictor(model1::model, Xt::Array{Float64, 2})
    # println("Predicting via model1 on stage 1... \n")
    A, = assemblePx(Xt, model1.pfuncs)
    A*model1.coef .+ model1.intercept
end

function predictor(model1::model, xi::Float64, λi::Float64, i::Int, ∇ = 2)
    # println("Predicting vector xn via model1 on stage 2... \n")
    X = model1.trainData.X
    y = model1.trainData.y
    tau = model1.tau
    ρ(x) = rho(x, tau)
    n, = size(X)
    msum = sum(model1.Mx, dims=2)
    u = model1.intercept
    yi = 0
    h = 1/λi
    Xi = X[:, i] .- xi
    Wi = ker(Xi, h)
	y_ = y .- u .- (msum .- model1.Mx[:,i])

    if ∇ == 2
        df = DataFrame([y_ ones(n) Xi].*Wi)
        noy = :x1
        nox = [:x2, :x3]
        res = localLinear_quantReg.qr_solver(df, noy, nox, tau)
        # println(res)
        yi =  res[2] 
    
    elseif ∇ == 1
        df = DataFrame([y_ ones(n)].*Wi)
        noy = :x1
        nox = [:x2]
        res = localLinear_quantReg.qr_solver(df, noy, nox, tau)
        # println(res)
        yi =  res[2] 

    elseif ∇ == 3
        df = DataFrame([y_ ones(n) Xi  Xi.^2].*Wi)
        noy = :x1
        nox = [:x2, :x3, :x4]
        res = localLinear_quantReg.qr_solver(df, noy, nox, tau)
        # println(res)
        yi =  res[2] 

    elseif ∇ == 4
        df = DataFrame([y_ ones(n) Xi  Xi.^2 Xi.^3].*Wi)
        noy = :x1
        nox = [:x2, :x3, :x4, :x5]
        res = localLinear_quantReg.qr_solver(df, noy, nox, tau)
        # println(res)
        yi =  res[2] 

    end
    yi
end

####
# Parallel on d
function predictor(model1::model, Xi::Array{Float64, 1}, λi::Float64, i::Int, ∇)
    # println("Predicting vector xn via model1 on stage 2... \n")
    X = model1.trainData.X
    n = length(Xi)
    yi = zeros(n)
    for k in 1:n
        xi = Xi[k]
        yi[k] = predictor(model1, xi, λi, i, ∇)
    end
    yi
end

# Parallel on n
function predictor(model1::model, Xn::Array{Float64, 1}, λ::Array{Float64, 1}, ∇)
    # println("Predicting vector xn via model1 on stage 2... \n")
    # X = model1.trainData.X
    d = length(Xn)
    yn = 0
    for k in 1:d
        xk = Xn[k]
        yn += predictor(model1, xk, λ[k], k, ∇)
    end
    yn
end

function predictor(model1::model, Xt::Array{Float64, 2}, λ::Array{Float64, 1}, ∇ = 2, sepy = false) # 之前是对d并行，最多只有d个线程，这里改成对n并F行
    # println("Predicting matrix Xt via model1 on stage 2... \n")
    n, d = size(Xt)
    u = model1.intercept
    sep_y = zeros(n, d)
    y = ones(n).*u

    for i in 1:n
        y[i] += predictor(model1, Xt[i, :], λ, ∇)
    end


    if sepy
        return sep_y
    else
        return y
    end
end

# SAE(model1::model, λ::Array{Float64, 1}, ∇) = sum(rho.(model1.trainData.y - predictor(model1, model1.trainData.X, λ, ∇), model1.tau))

SAE(model1::model, testData, λ::Array{Float64, 1}, ∇) = sum(rho.(testData.y - predictor(model1, testData.X, λ, ∇), model1.tau))

end # end module
# @Threads.threads 
# tasks = Vector{Task}(undef, d)

    # for i in 1:d
    #     tasks[i] = @spawn func(i)
    # end
    # for i in 1:d
    #     try tasks[i]
    #     catch e
    #         if isa(e, UndefRefError)
    #             continue
    #         else
    #             wait(task[i])
    #         end
    #     end
    # end
    # while count != d
    #     sleep(0.1)
    # end
    # wait(tasks[d])
